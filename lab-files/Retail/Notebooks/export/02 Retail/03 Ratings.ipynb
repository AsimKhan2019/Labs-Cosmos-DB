{"cells":[{"cell_type":"markdown","source":["# Calculate Ratings from Implicit Activity\nThis notebook will use the implict events captured in the `events` collection in Cosmos DB to calculate what a user would rate a given item, based on their actions. In other words it converts a users `buy`, `addToCart` and `details` actions into a numeric score for the item. The resulting user to item ratings matrix will be saved to the `ratings` collection in Cosmos DB.\n\nRun the following cell to retrieve the shared configuration values that point to your instance of Cosmos DB."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Shared-Configuration\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["readEventsConfig = {\n\"Endpoint\" : cosmos_db_endpoint,\n\"Masterkey\" : cosmos_db_master_key,\n\"Database\" : cosmos_db_database,\n\"Collection\" : \"events\",\n\"preferredRegions\" : \"West US\", \n\"SamplingRatio\" : \"1.0\",\n\"schema_samplesize\" : \"1000\",\n\"query_pagesize\" : \"2147483647\",\n}\n\nwriteRatingsConfig = {\n\"Endpoint\" : cosmos_db_endpoint,\n\"Masterkey\" : cosmos_db_master_key,\n\"Database\" : cosmos_db_database,\n\"Collection\" : \"ratings\"\n}"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Whenever you write data back to Cosmos DB, you will need to provide a schema for DataFrame to apply when writing. Run the following cell to define this schema object."],"metadata":{}},{"cell_type":"code","source":["# Schema used by the ratings collection\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\nratingsSchema = StructType([\n  StructField(\"userId\",StringType(),True),\n  StructField(\"itemId\",StringType(),True),\n  StructField(\"rating\",DoubleType(),True),\n  StructField(\"ratingTimeStamp\",StringType(),True),\n  StructField(\"_attachments\",StringType(),True),\n  StructField(\"_etag\",StringType(),True),\n  StructField(\"_rid\",StringType(),True),\n  StructField(\"_self\",StringType(),True),\n  StructField(\"_ts\",IntegerType(),True),\n])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["In addition to the Spark Connector for Cosmos DB, this notebook also uses the Azure Cosmos DB Python SDK. Run the following cell to install it."],"metadata":{}},{"cell_type":"code","source":["# import the Cosmos DB Python SDK\ndbutils.library.installPyPI('azure-cosmos', version='3.1.1')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Define the ratings calculation logic\n\nThe following cell defines the logic for calculating the rating between any two items, based on the historical user activity about each item.\n\nRun the following cell to define the logic."],"metadata":{}},{"cell_type":"code","source":["import os\nimport datetime\nfrom datetime import date, timedelta\nfrom collections import defaultdict\n\nw1 = 100\nw2 = 50\nw3 = 15\n\ndef truncate_collection(config):\n    # delete any existing ratings\n    from azure.cosmos import cosmos_client\n    database_link = 'dbs/' + config['Database']\n    collection_link = database_link + '/colls/' + config['Collection']\n    client = cosmos_client.CosmosClient(url_connection=config['Endpoint'], auth={'masterKey': config['Masterkey']})\n\n    documentlist = list(client.ReadItems(collection_link, {'maxItemCount':10}))\n\n    print('Found {0} documents'.format(documentlist.__len__()))\n\n    options = {}\n    options['enableCrossPartitionQuery'] = True\n    options['maxItemCount'] = 5\n\n    for doc in documentlist:\n        print('Deleting Document Id: {0}'.format(doc['id']))\n        docLink = collection_link + '/docs/' + doc['id']\n        options['partitionKey'] = doc['itemId']\n        client.DeleteItem(docLink, options)\n\n\ndef query_log_for_users():\n    return spark.sql(\"SELECT DISTINCT userId FROM events\")\n\n\ndef query_aggregated_log_data_for_user(userId):\n\n    user_events_summary = spark.sql(\"SELECT userId, contentId as itemId, event, count(event) as count FROM events WHERE userid = '{0}' GROUP BY userId, itemId, event\".format(userId))\n\n    return user_events_summary\n\n\ndef calculate_implicit_ratings_for_user(userId):\n\n    print(\"calculate_implicit_ratings_for_user : entered\")\n\n    data = query_aggregated_log_data_for_user(userId)\n  \n    #print(\"calculate_implicit_ratings_for_user : data = {0}\".format(data))\n\n    agg_data = dict()\n    max_rating = 0\n\n    for row in data.collect():\n        itemId = str(row['itemId'])\n        if itemId not in agg_data.keys():\n            agg_data[itemId] = defaultdict(int)\n\n        agg_data[itemId][row['event']] = row['count']\n\n    #print(\"calculate_implicit_ratings_for_user : agg_data = {0}\".format(agg_data))\n \n    ratings = dict()\n    for k, v in agg_data .items():\n\n        rating = w1 * v['buy'] + w2 * v['addToCart'] + w3 * v['details']\n        max_rating = max(max_rating, rating)\n\n        ratings[k] = rating\n\n    #print(\"calculate_implicit_ratings_for_user : user_id = {0}, max_rating = {1}\".format(user_id, max_rating))\n\n    for itemId in ratings.keys():\n        ratings[itemId] = 10 * ratings[itemId] / max_rating\n\n    #print(\"calculate_implicit_ratings_for_user : ratings = {0}\".format(ratings))\n\n    return ratings\n\n\ndef save_ratings(ratings, userId, config):\n    print(\"saving ratings for {}\".format(userId))\n    i = 0\n    \n    from pyspark.sql import Row\n    newRows = []\n    \n    for itemId, rating in ratings.items():     \n        if rating > 0:\n            newRows.append( \n              #userId:string, itemId:string, rating:double, ratingTimeStamp:string, _attachments:string, _etag:string, _rid:string, _self:string, _ts:integer\n              Row(userId, itemId, rating, str(datetime.datetime.utcnow()), None,None,None,None,None)\n            )\n\n    parallelizeRows = spark.sparkContext.parallelize(newRows)\n    new_documents = spark.createDataFrame(parallelizeRows, ratingsSchema)\n    new_documents.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"overwrite\").options(**config).save()\n\n\ndef calculate_ratings(writeRatingsConfig):\n    rows = query_log_for_users()\n    #display(rows)\n\n    for row in rows.collect():\n        userId = row['userId']\n        print(userId)\n        ratings = calculate_implicit_ratings_for_user(userId)\n        save_ratings(ratings, userId, writeRatingsConfig)\n        \n\n\n\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Run the following cell to calculate the ratings and store them in the `ratings` collection within Cosmos DB."],"metadata":{}},{"cell_type":"code","source":["print(\"Deleting existing implicit ratings...\")\ntruncate_collection(writeRatingsConfig)\n\n# Connect via Spark connector to create Spark DataFrame\nevents_df = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**readEventsConfig).load()\nevents_df.createOrReplaceTempView(\"events\")\n\nprint(\"Calculating implicit ratings...\")\ncalculate_ratings(writeRatingsConfig)\n\nratings_df = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**writeRatingsConfig).load()\nratings_df.createOrReplaceTempView(\"ratings\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Run the following cell to view the statistics about the rating values that were generated. Notice they run between very close to 0 and 10."],"metadata":{}},{"cell_type":"code","source":["display(ratings_df.describe(\"rating\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Run the following cell to query the temporary view representing the ratings data using Spark SQL."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT itemId, userId, rating FROM ratings"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["You are finished with this notebook and can return to the lab guide."],"metadata":{}}],"metadata":{"name":"03 Ratings","notebookId":2023663018381832},"nbformat":4,"nbformat_minor":0}
