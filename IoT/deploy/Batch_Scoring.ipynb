{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verify Python libraries are installed\n",
        "Note that your Synapse Spark pool includes all the libraries required to run this notebook. They were added during the pool creation by using the correct requirements.txt file.\n",
        "\n",
        "The libraries installed are:\n",
        "```python\n",
        "    numpy==1.17.1\n",
        "    pandas==0.24.2\n",
        "    idna==2.5\n",
        "    scipy==1.3.1\n",
        "    azureml-sdk==1.3.0\n",
        "    azureml-automl-core==1.3.0\n",
        "    azureml-automl-runtime==1.2.0\n",
        "```\n",
        "\n",
        "Synapse Spark pools already have the required libraries to connect to Cosmos DB operational and analytical storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "Azure ML SDK Version: 1.6.0\nSciPy Version:  1.1.0"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import azureml\n",
        "from azureml.core import Run\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "import scipy\n",
        "\n",
        "# Verify versions of key libraries\n",
        "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
        "print(\"Azure ML SDK Version:\", azureml.core.VERSION)\n",
        "print(\"SciPy Version: \", scipy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure access to the Azure Machine Learning resources\n",
        "\n",
        "## Configure Service Principal authentication following the instructions here: [Setup Service Principal Authentication](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?view=azure-ml-py#set-up-service-principal-authentication).\n",
        "\n",
        "Use the JSON output from the commands in the above link to retrieve the values needed for `tenant_id`, `service_principal_id`, and `service_principal_password` in the next cell.\n",
        "\n",
        "Note: if the Azure account you are using has access to multiple Azure subscriptions, **make sure you run CLI commands in the correct Azure subscription**. You can set the default subscription to the one you are using for the lab/demo with the Azure CLI command `az account set`.\n",
        "\n",
        "Reference: https://docs.microsoft.com/cli/azure/account#az-account-set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variables\n",
        "\n",
        "Provide values for the following variables which will be used throughout the rest of this notebook."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Provide the Subscription ID of the Azure subscription you are using for the lab/demo\n",
        "subscription_id = \"220fc532-6091-423c-8ba0-66c2397d591b\"\n",
        "\n",
        "# Resource Group name where your lab/demo resources are deployed\n",
        "resource_group = \"pz-iot-demo-20200618-1\"\n",
        "\n",
        "# Azure Machine Learning Workspace name and Azure region\n",
        "# Get these from the Azure ML workspace Overview in your Resource Group\n",
        "workspace_name = \"Cosmos-DB-IoT-ML-vhdozdcujguho\"\n",
        "workspace_region = \"East US\"\n",
        "\n",
        "# Values from `Setup Service Principal Authentication` in the above cell\n",
        "# For reference, SP name you created (not needed in a variable): pz-ml-auth\n",
        "tenant_id = \"\" # Use \"tenantId\" value\n",
        "service_principal_id = \"\" # Use \"clientId\" value\n",
        "service_principal_password = \"\" # Use \"clientSecret\" value\n",
        "\n",
        "# Pre-trained ML model\n",
        "# Update for final release\n",
        "# pkl_url = \"https://github.com/AzureCosmosDB/scenario-based-labs/blob/master/IoT/deploy/modelv3.pkl?raw=true\"\n",
        "pkl_url = \"https://github.com/plzm/scenario-based-labs/blob/iot-2020/IoT/deploy/modelv3.pkl?raw=true\"\n",
        "local_folder = \"models\"\n",
        "local_path = local_folder+\"/modelv3.pkl\"\n",
        "model_name = \"batt-cycles-7\"\n",
        "\n",
        "# Cosmos DB\n",
        "# Change this to the Azure region to which you deployed your lab/demo\n",
        "cosmos_db_region = \"East US\"\n",
        "cosmos_db_database = \"ContosoAuto\"\n",
        "cosmos_db_container_metadata = \"metadata\"\n",
        "cosmos_db_container_maintenance = \"maintenance\"\n",
        "\n",
        "synapse_cosmos_db_linked_service = \"CosmosDbIoTLab\"\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Scoring data\n",
        "In this notebook, you will use a forecasting model to determine if the battery will need replacement within the next 30 days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "\n",
        "sp = ServicePrincipalAuthentication(\n",
        "    tenant_id=tenant_id,\n",
        "    service_principal_id=service_principal_id,\n",
        "    service_principal_password=service_principal_password)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "{'id': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourceGroups/pz-iot-demo-20200618-1/providers/Microsoft.MachineLearningServices/workspaces/Cosmos-DB-IoT-ML-vhdozdcujguho', 'name': 'Cosmos-DB-IoT-ML-vhdozdcujguho', 'location': 'eastus', 'type': 'Microsoft.MachineLearningServices/workspaces', 'sku': 'Basic', 'workspaceid': 'aa21de48-1752-4fd5-85c1-308ec85899bb', 'description': '', 'friendlyName': 'Cosmos-DB-IoT-ML-vhdozdcujguho', 'creationTime': '2020-06-18T12:38:32.9520027+00:00', 'keyVault': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourcegroups/pz-iot-demo-20200618-1/providers/microsoft.keyvault/vaults/iot-vault-vhdozdcujguho', 'applicationInsights': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourcegroups/pz-iot-demo-20200618-1/providers/microsoft.insights/components/cosmos-db-iot-insights-vhdozdcujguho', 'identityPrincipalId': 'aaade69b-2856-40ca-9c0f-bd1643b4ae1e', 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47', 'identityType': 'SystemAssigned', 'storageAccount': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourcegroups/pz-iot-demo-20200618-1/providers/microsoft.storage/storageaccounts/mlstorevhdozdcujguho', 'hbiWorkspace': False, 'imageBuildCompute': '', 'discoveryUrl': 'https://eastus.experiments.azureml.net/discovery'}"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# By using the exist_ok param, if the workspace already exists we get a reference to the existing workspace\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.get(\n",
        "    name=workspace_name, \n",
        "    auth=sp,\n",
        "    subscription_id=subscription_id)\n",
        "\n",
        "ws.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the pre-trained model\n",
        "A pre-trained model has been made available in a public Azure Storage account. Run the following cell to download the model and then register it as a model within your Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "Downloading the pre-trained model...\nDownload complete.\nUploading and registering model...\nRegistering model batt-cycles-7"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from azureml.core import Model\n",
        "\n",
        "print(\"Downloading the pre-trained model...\")\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "urllib.request.urlretrieve(pkl_url, local_path)\n",
        "\n",
        "print(\"Download complete.\")\n",
        "\n",
        "print(\"Uploading and registering model...\")\n",
        "registered_model = Model.register(\n",
        "    model_path=local_path, \n",
        "    model_name=model_name, \n",
        "    workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following to retrieve the model from your Azure Machine Learning workspace, and inspect some of its properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "Model saved to  azureml-models/batt-cycles-7/7/modelv3.pkl\nModel loaded."
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from azureml.core.model import Model\n",
        "from sklearn.externals import joblib\n",
        "from azureml.train import automl\n",
        "\n",
        "model_path = Model.get_model_path(model_name=model_name, _workspace=ws)\n",
        "print(\"Model saved to \", model_path)\n",
        "model = joblib.load(model_path)\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data from Cosmos DB to batch score it\n",
        "Run the following cells to query Cosmos DB Analytical store, prepare the data using SQL queries and then surface the data as temporary views."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Temp View\n",
        "Now we register the view required to create the dataset that will be used to make the predictions. Notice how you are now capable to join data from multiple Cosmos DB containers.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "# vehicle_metadata_df = spark.read.cosmos_olap('metadata').createOrReplaceTempView(\"metadata\")\n",
        "\n",
        "vehicle_metadata_df = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", synapse_cosmos_db_linked_service)\\\n",
        "    .option(\"spark.cosmos.container\", cosmos_db_container_metadata)\\\n",
        "    .load()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "187136\nroot\n |-- _rid: string (nullable = true)\n |-- _ts: long (nullable = true)\n |-- id: string (nullable = true)\n |-- _etag: string (nullable = true)\n |-- partitionKey: string (nullable = true)\n |-- entityType: string (nullable = true)\n |-- vin: string (nullable = true)\n |-- lastServiceDate: string (nullable = true)\n |-- batteryAgeDays: long (nullable = true)\n |-- batteryRatedCycles: long (nullable = true)\n |-- lifetimeBatteryCyclesUsed: double (nullable = true)\n |-- averageDailyTripDuration: double (nullable = true)\n |-- batteryFailurePredicted: boolean (nullable = true)\n |-- stateVehicleRegistered: string (nullable = true)\n |-- customer: string (nullable = true)\n |-- description: integer (nullable = true)\n |-- status: string (nullable = true)\n |-- deliveryDueDate: string (nullable = true)\n |-- packages: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- timestamp: string (nullable = true)\n |-- tripId: string (nullable = true)\n |-- consignmentId: string (nullable = true)\n |-- height: double (nullable = true)\n |-- length: double (nullable = true)\n |-- width: double (nullable = true)\n |-- grossWeight: double (nullable = true)\n |-- storageTemperature: long (nullable = true)\n |-- highValue: boolean (nullable = true)\n |-- trip: struct (nullable = true)\n |    |-- tripId: string (nullable = true)\n |    |-- vin: string (nullable = true)\n |    |-- plannedTripDistance: double (nullable = true)\n |-- consignment: struct (nullable = true)\n |    |-- consignmentId: string (nullable = true)\n |    |-- customer: string (nullable = true)\n |    |-- deliveryDueDate: string (nullable = true)\n |-- plannedTripDistance: double (nullable = true)\n |-- location: string (nullable = true)\n |-- odometerBegin: long (nullable = true)\n |-- odometerEnd: double (nullable = true)\n |-- temperatureSetting: long (nullable = true)\n |-- tripStarted: string (nullable = true)\n |-- tripEnded: string (nullable = true)\n |-- engineTemperature: double (nullable = true)\n |-- speed: double (nullable = true)\n |-- refrigerationUnitKw: double (nullable = true)\n |-- refrigerationUnitTemp: double (nullable = true)\n |-- engineTempAnomaly: long (nullable = true)\n |-- oilAnomaly: long (nullable = true)\n |-- aggressiveDriving: long (nullable = true)\n |-- refrigerationTempAnomaly: long (nullable = true)\n |-- snapshot: string (nullable = true)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "print(vehicle_metadata_df.count())\n",
        "\n",
        "vehicle_metadata_df.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicle_metadata_df.createOrReplaceTempView(\"metadata\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
          "traceback": [
            "AnalysisException : 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 767, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
            "pyspark.sql.utils.AnalysisException: 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "metadata = spark.sql(\"\"\"\n",
        "    SELECT * FROM metadata LIMIT 10\n",
        "    \"\"\")\n",
        "\n",
        "metadata.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Scoring dataset\n",
        "Now we are ready to use the previously created view to generate the final dataset"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
          "traceback": [
            "AnalysisException : 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 767, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
            "pyspark.sql.utils.AnalysisException: 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "trips_clean = spark.sql(\"\"\"\n",
        "    SELECT  vin, \n",
        "            to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripEnded, \n",
        "            to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripStarted, \n",
        "            ((unix_timestamp(to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")) - \n",
        "                unix_timestamp(to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")))/60.0) as tripDurationMinutes\n",
        "    FROM metadata\n",
        "    WHERE   entityType = 'Trip' \n",
        "            AND (tripStarted is not null AND tripStarted <> '0' AND tripStarted <> '') \n",
        "            AND (tripEnded is not null AND tripEnded <> '0' AND tripEnded <> '')\n",
        "    \"\"\")\n",
        "\n",
        "trips_clean.createOrReplaceTempView(\"trips_clean\")\n",
        "trips_clean.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
          "traceback": [
            "AnalysisException : 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 767, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
            "pyspark.sql.utils.AnalysisException: 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "vehicles_raw = spark.sql(\"\"\"\n",
        "    SELECT vin, batteryAgeDays, batteryRatedCycles, lifetimeBatteryCyclesUsed \n",
        "    FROM metadata \n",
        "    WHERE entityType ='Vehicle'\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_raw.createOrReplaceTempView(\"vehicles_raw\")\n",
        "vehicles_raw.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
          "traceback": [
            "AnalysisException : 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 767, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
            "pyspark.sql.utils.AnalysisException: 'java.lang.RuntimeException: Operation failed: \"This request is not authorized to perform this operation using this permission.\", 403, HEAD, https://synsavhdozdcujguho.dfs.core.windows.net/workspace/tmp/hive?upn=false&timeout=90;'\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "vehicles_batch = spark.sql(\"\"\"\n",
        "    SELECT  v.vin as vin, \n",
        "            to_date(t.tripEnded, 'yyyy-MM-dd') as tripEnded, \n",
        "            t.tripDurationMinutes, \n",
        "            v.batteryAgeDays, \n",
        "            v.batteryRatedCycles, \n",
        "            v.lifetimeBatteryCyclesUsed \n",
        "    FROM    vehicles_raw v \n",
        "    INNER JOIN trips_clean t \n",
        "        ON v.vin = t.vin\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_batch.createOrReplaceTempView(\"vehicles_batch\")\n",
        "vehicles_batch.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cells to convert the Spark DataFrame to a Pandas DataFrame for use with the pre-created model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "spark_df = spark.sql(\"SELECT cast(tripEnded as string) as date, batteryAgeDays as battery_Age_Days, tripDurationMinutes as daily_Trip_Duration, lifetimeBatteryCyclesUsed, batteryRatedCycles, vin from vehicles_batch v\")\n",
        "pd_df = spark_df.toPandas()\n",
        "pd_df['date'] = pd.to_datetime(pd_df['date']) # Added to address Spark Date to Pandas date conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the scoring logic\n",
        "The following cell will apply the model and return a prediction for whether or not maintenance is required.\n",
        "\n",
        "Run the following cell to define the helper method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def predict_maintenance(row):\n",
        "    # from azureml.train import automl\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "    predict_needs_service = 0\n",
        "    \n",
        "    startday = row[\"battery_Age_Days\"]\n",
        "    dailytripduration = row[\"daily_Trip_Duration\"]\n",
        "    current_cycles = row[\"lifetimeBatteryCyclesUsed\"]\n",
        "    rated_lifetime_cycles = row[\"batteryRatedCycles\"]\n",
        "\n",
        "    dayslist = range(startday, startday+30)\n",
        "    pds_df = pd.DataFrame({'battery_Age_Days': dayslist, 'daily_Trip_Duration': dailytripduration})\n",
        "\n",
        "    y_Pred = reg.predict(np.array(pds_df))\n",
        "    total_cycles_next_30_days = y_Pred[[29,]][0][0]\n",
        "\n",
        "    if current_cycles + total_cycles_next_30_days > rated_lifetime_cycles:\n",
        "        predict_needs_service = 1\n",
        "\n",
        "    return predict_needs_service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the predictions by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "predictions = pd_df.apply(predict_maintenance, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, run the following cell to examine the predication by `VIN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "batch_predictions_pdf = pd.DataFrame({\"vin\": pd_df[\"vin\"], \"serviceRequired\":predictions})\n",
        "batch_predictions_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write the predictions back to Cosmos DB\n",
        "Now you will save the previously created predictions DataFrame back to the `maintenance` collection in Cosmos DB.\n",
        "\n",
        "Run the following cells to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Retrieve connection string and key from LinkService\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "connection_string = token_library.getConnectionString(synapse_cosmos_db_linked_service)\n",
        "matchObj = re.match( r'AccountEndpoint=(.*);Database=(.*);AccountKey=\"(.*)\";', connection_string, re.M|re.I)\n",
        "endpoint = matchObj.group(1)\n",
        "masterkey = matchObj.group(3)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "maintReadConfig = {\n",
        "    \"Endpoint\" : endpoint,\n",
        "    \"Masterkey\" : masterkey,\n",
        "    \"Database\" : cosmos_db_database,\n",
        "    \"Collection\" : cosmos_db_container_maintenance\n",
        "}\n",
        "\n",
        "maint = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**maintReadConfig).load()\n",
        "maint.createOrReplaceTempView(\"maintenance\")\n",
        "\n",
        "writeConfig = {\n",
        "    \"Endpoint\" : endpoint,\n",
        "    \"Masterkey\" : masterkey,\n",
        "    \"Database\" : cosmos_db_database,\n",
        "    \"Collection\" : cosmos_db_container_maintenance,\n",
        "    \"Upsert\" : \"false\"\n",
        "}\n",
        "\n",
        "# Schema used by the maintenance collection\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
        "maintSchema = StructType([\n",
        "  StructField(\"vin\",StringType(),True),\n",
        "  StructField(\"serviceRequired\",IntegerType(),True),\n",
        "  StructField(\"id\",StringType(),True),\n",
        "  StructField(\"_attachments\",StringType(),True),\n",
        "  StructField(\"_etag\",StringType(),True),\n",
        "  StructField(\"_rid\",StringType(),True),\n",
        "  StructField(\"_self\",StringType(),True),\n",
        "  StructField(\"_ts\",IntegerType(),True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# delete any existing maintenance predictions\n",
        "from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
        "\n",
        "client = CosmosClient(endpoint, credential=masterkey)\n",
        "database = client.get_database_client(cosmos_db_database)\n",
        "container = database.get_container_client(cosmos_db_container_maintenance)\n",
        "\n",
        "for item in container.query_items(query='SELECT * FROM c',\n",
        "                                  enable_cross_partition_query=True):\n",
        "    print('Deleting Document Id: {0}'.format(item['id']))\n",
        "    container.delete_item(item, partition_key=item['vin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# write the new prediction out to Cosmos DB\n",
        "batch_predictions = spark.createDataFrame(batch_predictions_pdf)\n",
        "batch_predictions.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"overwrite\").options(**writeConfig).save()"
      ]
    }
  ],
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}