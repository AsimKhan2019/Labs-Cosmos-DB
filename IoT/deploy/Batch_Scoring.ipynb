{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Verify Python libraries are installed\n",
        "Note that your Synapse Spark pool includes all the libraries required to run this notebook. They were added during the pool creation by using the correct requirements.txt file.\n",
        "\n",
        "The libraries installed are:\n",
        "```python\n",
        "    numpy==1.17.1\n",
        "    pandas==0.24.2\n",
        "    idna==2.5\n",
        "    scipy==1.3.1\n",
        "    azureml-sdk==1.3.0\n",
        "    azureml-automl-core==1.3.0\n",
        "    azureml-automl-runtime==1.2.0\n",
        "```\n",
        "\n",
        "Synapse Spark pool already have the required libraries to connect to Cosmos DB operational and analytical storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Azure ML SDK Version: 1.3.0\nSciPy Version:  1.1.0"
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import azureml\n",
        "from azureml.core import Run\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "import scipy\n",
        "\n",
        "# Verify versions of key libraries\n",
        "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
        "print(\"Azure ML SDK Version:\", azureml.core.VERSION)\n",
        "print(\"SciPy Version: \", scipy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Scoring data\n",
        "In this notebook, you will use apply forecasting model you created previously to determine if the battery will be in need of replacement within the next 30 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure access to the Azure Machine Learning resources\n",
        "To begin, you will need to provide the following information about your Azure Subscription.\n",
        "\n",
        "**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** You should already have the Azure Machine Learning service workspace in your lab resource group. If not, the values you enter will be used to create a new one (note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace)).\n",
        "\n",
        "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
        "\n",
        "To get these values, do the following:  \n",
        "1. Navigate to the Azure Portal and login with the credentials provided.  \n",
        "2. From the left hand menu, under Favorites, select `Resource Groups`.  \n",
        "3. In the list, select the resource group used for the lab.  \n",
        "4. Open your Azure Machine Learning service workspace.\n",
        "\n",
        "  - **If this does not yet exist**, you can retrieve the `subscription_id`, `resource_group`, and `workspace_region` values from the resource group's Overview blade. You will need to make up your own `workspace_name` value, such as \"iot-aml-ws-YOUR_INITIALS\". **Set to the existing workspace name if it already exists**.   \n",
        "\n",
        "\n",
        "5. The requested values should be in the Overview blade."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Finished setting Azure Machine Learning service variables."
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "#Provide the Subscription ID of your existing Azure subscription\n",
        "subscription_id = \"220fc532-6091-423c-8ba0-66c2397d591b\"\n",
        "\n",
        "#Provide values for the existing Resource Group \n",
        "resource_group = \"iot-lab-2020\"\n",
        "\n",
        "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
        "workspace_name = \"Cosmos-DB-IoT-ML-2klrbk7bxl3tk\"\n",
        "workspace_region = \"East US\"\n",
        "\n",
        "print(\"Finished setting Azure Machine Learning service variables.\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure access to the Azure Machine Learning resources\n",
        "\n",
        "Run the following cells to connect to your **Azure Machine Learning Workspace**  Make sure the Workspace already exist and your Service Principal has been configured to use it followig the instrucitons defined in [Setup Service Principal Authentication](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?view=azure-ml-py#set-up-service-principal-authentication)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "\n",
        "# How do I get this from KeyVault?\n",
        "sp = ServicePrincipalAuthentication(\n",
        "        tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\", # service principal tenantID\n",
        "        service_principal_id=\"299abf99-3a3e-4ede-95e6-5c76944a5c4f\", # service principal clientId\n",
        "        service_principal_password=\"1d71ab00-4f54-47d5-8f43-2d093517e017\") # service principal clientSecret "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'id': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourceGroups/iot-lab-2020/providers/Microsoft.MachineLearningServices/workspaces/Cosmos-DB-IoT-ML-2klrbk7bxl3tk', 'name': 'Cosmos-DB-IoT-ML-2klrbk7bxl3tk', 'location': 'eastus', 'type': 'Microsoft.MachineLearningServices/workspaces', 'sku': 'Enterprise', 'workspaceid': '13d4628c-5afc-40c3-9a7a-ef09ad68a159', 'description': '', 'friendlyName': '', 'creationTime': '2020-04-13T17:31:53.6662789+00:00', 'containerRegistry': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourceGroups/iot-lab-2020/providers/Microsoft.ContainerRegistry/registries/cosmosdbiotm2bc27104', 'keyVault': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourcegroups/iot-lab-2020/providers/microsoft.keyvault/vaults/iot-vault-2klrbk7bxl3tk', 'applicationInsights': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourcegroups/iot-lab-2020/providers/microsoft.insights/components/cosmos-db-iot-insights-2klrbk7bxl3tk', 'identityPrincipalId': '6e0d519d-2cac-4cac-aa6f-456d1ea305e5', 'identityTenantId': '72f988bf-86f1-41af-91ab-2d7cd011db47', 'identityType': 'SystemAssigned', 'storageAccount': '/subscriptions/220fc532-6091-423c-8ba0-66c2397d591b/resourcegroups/iot-lab-2020/providers/microsoft.storage/storageaccounts/mlstore2klrbk7bxl3tk', 'hbiWorkspace': False, 'imageBuildCompute': '', 'discoveryUrl': 'https://eastus.experiments.azureml.net/discovery'}"
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# By using the exist_ok param, if the worskpace already exists we get a reference to the existing workspace\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.get(\n",
        "        name=workspace_name, \n",
        "        auth=sp,\n",
        "        subscription_id=subscription_id)\n",
        "ws.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the pre-trained model\n",
        "A pre-trained models has been made available in a public Azure Storage account. Run the following cell to download the model and then register it as a model within your Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Downloading the pre-trained model...\nDownload complete.\nUploading and registering model...\nRegistering model batt-cycles-7"
          },
          "execution_count": 17,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "print(\"Downloading the pre-trained model...\")\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "# Update Code to download file from New Repo\n",
        "#urllib.request.urlretrieve('https://github.com/AzureCosmosDB/scenario-based-labs/tree/master/IoT/deploy/modelv3.pkl', 'models/modelv3.pkl')\n",
        "print(\"Download complete.\")\n",
        "\n",
        "print(\"Uploading and registering model...\")\n",
        "registered_model = Model.register(model_path=\"models/modelv3.pkl\", \n",
        "                                  model_name=\"batt-cycles-7\", \n",
        "                                  workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following to retrieve the model from your Azure Machine Learning workspace, and inspect some of its properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Model saved to  azureml-models/batt-cycles-7/3/modelv3.pkl\nModel loaded."
          },
          "execution_count": 18,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from azureml.core.model import Model\n",
        "from sklearn.externals import joblib\n",
        "from azureml.train import automl\n",
        "\n",
        "model_path = Model.get_model_path(model_name = 'batt-cycles-7', _workspace=ws)\n",
        "print(\"Model saved to \", model_path)\n",
        "model = joblib.load(model_path)\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data from Cosmos DB to batch score it\n",
        "Run the following cells to query Cosmos DB Analytical store, prepare the data using SQL queries and then surface the data as temporary views."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registering Helper Function\n",
        "This function makes it easier to create dataframe based on the Analytical store containers"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pyspark\n",
        "\n",
        "def _cosmos_olap(self, collection):\n",
        "    cdb_analytical_config = {\n",
        "    \"spark.cosmos.synapse.linkedServiceName\" : \"CosmosDbIoTLab\",\n",
        "    \"spark.cosmos.region\" : \"eastus2\",\n",
        "    \"spark.cosmos.databaseName\" : \"ContosoAuto\",\n",
        "    \"spark.cosmos.containerName\" : collection\n",
        "    }\n",
        "    return self.format('com.microsoft.azure.cosmos.analytics.spark.connector.CosmosSource')\\\n",
        "        .options(**cdb_analytical_config)\\\n",
        "        .load()\n",
        "    \n",
        "setattr(pyspark.sql.readwriter.DataFrameReader, 'cosmos_olap', _cosmos_olap)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Temp View\n",
        "Now we register the view required to create the dataset that will use to make the predictions. Notice how you are now capable to join data from multiple Cosmos DB containers\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "vechicle_metadata_df = spark.read.cosmos_olap('metadata').createOrReplaceTempView(\"metadata\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Scoring dataset\n",
        "Now we are ady to use the previously created view to generate the final dataset"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- vin: string (nullable = true)\n |-- tripEnded: timestamp (nullable = true)\n |-- tripStarted: timestamp (nullable = true)\n |-- tripDurationMinutes: decimal(27,6) (nullable = true)"
          },
          "execution_count": 9,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "trips_clean = spark.sql(\"\"\"\n",
        "    SELECT  vin, \n",
        "            to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripEnded, \n",
        "            to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripStarted, \n",
        "            ((unix_timestamp(to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")) - \n",
        "                unix_timestamp(to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")))/60.0) as tripDurationMinutes\n",
        "    FROM metadata\n",
        "    WHERE   entityType = 'Trip' \n",
        "            AND (tripStarted is not null AND tripStarted <> '0' AND tripStarted <> '') \n",
        "            AND (tripEnded is not null AND tripEnded <> '0' AND tripEnded <> '')\n",
        "    \"\"\")\n",
        "\n",
        "trips_clean.createOrReplaceTempView(\"trips_clean\")\n",
        "trips_clean.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- vin: string (nullable = true)\n |-- batteryAgeDays: long (nullable = true)\n |-- batteryRatedCycles: long (nullable = true)\n |-- lifetimeBatteryCyclesUsed: double (nullable = true)"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "vehicles_raw = spark.sql(\"\"\"\n",
        "    SELECT vin, batteryAgeDays, batteryRatedCycles, lifetimeBatteryCyclesUsed \n",
        "    FROM metadata \n",
        "    WHERE entityType ='Vehicle'\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_raw.createOrReplaceTempView(\"vehicles_raw\")\n",
        "vehicles_raw.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- vin: string (nullable = true)\n |-- tripEnded: date (nullable = true)\n |-- tripDurationMinutes: decimal(27,6) (nullable = true)\n |-- batteryAgeDays: long (nullable = true)\n |-- batteryRatedCycles: long (nullable = true)\n |-- lifetimeBatteryCyclesUsed: double (nullable = true)"
          },
          "execution_count": 11,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "vehicles_batch = spark.sql(\"\"\"\n",
        "    SELECT  v.vin as vin, \n",
        "            to_date(t.tripEnded, 'yyyy-MM-dd') as tripEnded, \n",
        "            t.tripDurationMinutes, \n",
        "            v.batteryAgeDays, \n",
        "            v.batteryRatedCycles, \n",
        "            v.lifetimeBatteryCyclesUsed \n",
        "    FROM    vehicles_raw v \n",
        "    INNER JOIN trips_clean t \n",
        "        ON v.vin = t.vin\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_batch.createOrReplaceTempView(\"vehicles_batch\")\n",
        "vehicles_batch.printSchema()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cells to convert the Spark DataFrame to a Pandas DataFrame for use with the pre-created model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "spark_df = spark.sql(\"SELECT cast(tripEnded as string) as date, batteryAgeDays as battery_Age_Days, tripDurationMinutes as daily_Trip_Duration, lifetimeBatteryCyclesUsed, batteryRatedCycles, vin from vehicles_batch v\")\n",
        "pd_df = spark_df.toPandas()\n",
        "pd_df['date'] = pd.to_datetime(pd_df['date']) # Added to address Spark Date to Pandas date conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the scoring logic\n",
        "The following cell will apply the model and return a prediction for whether or not maintenance is required.\n",
        "\n",
        "Run the following cell to define the helper method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {},
      "source": [
        "def predict_maintenance(row):\n",
        "    # from azureml.train import automl\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "    predict_needs_service = 0\n",
        "    \n",
        "    startday = row[\"battery_Age_Days\"]\n",
        "    dailytripduration = row[\"daily_Trip_Duration\"]\n",
        "    current_cycles = row[\"lifetimeBatteryCyclesUsed\"]\n",
        "    rated_lifetime_cycles = row[\"batteryRatedCycles\"]\n",
        "\n",
        "    dayslist = range(startday, startday+30)\n",
        "    pds_df = pd.DataFrame({'battery_Age_Days': dayslist, 'daily_Trip_Duration': dailytripduration})\n",
        "\n",
        "    y_Pred = reg.predict(np.array(pds_df))\n",
        "    total_cycles_next_30_days = y_Pred[[29,]][0][0]\n",
        "\n",
        "    if current_cycles + total_cycles_next_30_days > rated_lifetime_cycles:\n",
        "        predict_needs_service = 1\n",
        "\n",
        "    return predict_needs_service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate the predictions by running the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "predictions = pd_df.apply(predict_maintenance, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, run the following cell to examine the predication by `VIN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "vin  serviceRequired\n0    QRD1S64DT4QIBM7AT                1\n1    IZGBOUOH0QDT7KX44                0\n2    CMJT8KNLDLWSSYAY8                1\n3    R3L4HK7T3NX7QVKEN                1\n4    5KIJ2LBT9NBXRYNJ1                0\n5    XF93A5HNUMQF3W7XN                1\n6    BTJWLTKYWTYE5QNYD                1\n7    H9B2RLTFU2H2I7ZZA                0\n8    PTZYTOTXBWVWBAJVG                1\n9    5Y7C4X1AW9OD6YJ5H                1\n10   OXPVYJT8F5CV0QETY                0\n11   TBMYQQ5TJDC85HHHD                0\n12   K9QF880KTD5NDKG0M                0\n13   8R5D8PMU2LJP25G25                1\n14   DZ0JN3HME3OKBBFYU                1\n15   PB2GAMT1UBQC0N2BW                0\n16   SQSP37SUBMYRE39P0                1\n17   HWNOTJPA7R5PBDWLZ                0\n18   DRZQCEQTOITFGYE1F                1\n19   GS7OYBBL6M7ENHIK5                1\n20   1UGMYO6ZDQ3KC72HT                0\n21   8RRMNKA6SN8JH8R6M                1\n22   VRJXVUQUWLMYNIC8X                1\n23   6WBGQ85RXT2YDTPVP                0\n24   C3TJEP9O4OHLFOYEK                1\n25   96E454V8GFVILTWSS                0\n26   HM3MLK42H5QEJE660                0\n27   3BP7P0NENUH9RI3M6                0\n28   FJN5UWII43ECSW9D8                0\n29   P32QUJV32R5ZOCLIB                1\n..                 ...              ...\n80   HIY403UZ5QSU9D9T1                1\n81   4R30FOR7NUOBL05GJ                0\n82   D7GARWGXTUUY80PLB                1\n83   TV6LN1B3KPQJEH1TZ                1\n84   2NNBYPLXMPYIGE1T5                1\n85   870I604O4BXFLA721                0\n86   II2VF0IYP5IAF0BS8                1\n87   F7SFZ0MU1XFQG9077                1\n88   SNWXKXTD3LIII3N96                1\n89   P1H6Z2D0ETZHF7PIA                1\n90   T6MGWM3HFCT09DWAN                1\n91   K0Z9D5WQV51K1D9DD                0\n92   AKYYQ91FZHZPZPW0T                0\n93   2LY9PP21E94R5IFMW                0\n94   449PLM0RCNHZ6YHO5                0\n95   HPFR22VNYIBN6073Y                0\n96   2UM8372QJ1AUJ4V9H                0\n97   RZAW4278UQ6HPF19Q                0\n98   9RO3097X9OREKUP5G                0\n99   IZ744LDZ0LJQ06ERN                1\n100  T6EX309MJUZTLEWEG                1\n101  ANAQ2CSXPZV04SSB3                0\n102  9K4NASRJYRC5IGE5F                1\n103  0LYX3XH2KQYH4N1HH                1\n104  I7W8KMBD4L3XHBI96                1\n105  F810N41O2NRBJHKBY                0\n106  DK6JW0RNF0G9PO2FJ                1\n107  CUEHKXE2HHXAKJWY5                1\n108  B325URQ271DPK9FSU                1\n109  BTHD3UQJMH9C06D6N                0\n\n[110 rows x 2 columns]"
          },
          "execution_count": 42,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "batch_predictions_pdf = pd.DataFrame({\"vin\": pd_df[\"vin\"], \"serviceRequired\":predictions})\n",
        "batch_predictions_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write the predictions back to Cosmos DB\n",
        "Now you will save the previously created predictions DataFrame back to the `maintenance` collection in Cosmos DB.\n",
        "\n",
        "Run the following cells to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Retrieve conneciton string and key from LinkService\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "connection_string = token_library.getConnectionString('CosmosDbIoTLab')\n",
        "matchObj = re.match( r'AccountEndpoint=(.*);Database=(.*);AccountKey=\"(.*)\";', connection_string, re.M|re.I)\n",
        "endpoint = matchObj.group(1)\n",
        "masterkey = matchObj.group(3)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "maintReadConfig = {\n",
        "    \"Endpoint\" : endpoint,\n",
        "    \"Masterkey\" : masterkey,\n",
        "    \"Database\" : \"ContosoAuto\",\n",
        "    \"Collection\" : \"maintenance\"\n",
        "    }\n",
        "\n",
        "maint = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**maintReadConfig).load()\n",
        "maint.createOrReplaceTempView(\"maintenance\")\n",
        "\n",
        "writeConfig = {\n",
        "    \"Endpoint\" : endpoint,\n",
        "    \"Masterkey\" : masterkey,\n",
        "    \"Database\" : \"ContosoAuto\",\n",
        "    \"Collection\" : \"maintenance\",\n",
        "    \"Upsert\" : \"false\"\n",
        "    }\n",
        "\n",
        "# Schema used by the maintenance collection\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
        "maintSchema = StructType([\n",
        "  StructField(\"vin\",StringType(),True),\n",
        "  StructField(\"serviceRequired\",IntegerType(),True),\n",
        "  StructField(\"id\",StringType(),True),\n",
        "  StructField(\"_attachments\",StringType(),True),\n",
        "  StructField(\"_etag\",StringType(),True),\n",
        "  StructField(\"_rid\",StringType(),True),\n",
        "  StructField(\"_self\",StringType(),True),\n",
        "  StructField(\"_ts\",IntegerType(),True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Deleting Document Id: 1668a5b9-c1e8-4db8-aa7e-83fc89486d2b\nDeleting Document Id: 9be8fea7-5dd3-4b6e-9e4d-4bd60ce35a10\nDeleting Document Id: 78c16daa-9675-4e70-b760-3610e2fda4ee\nDeleting Document Id: db237a88-82b4-4135-b95e-991b0b6e0fd4\nDeleting Document Id: d4f73293-dfbb-4eee-ac68-31f787baf752\nDeleting Document Id: f104b09f-4646-485c-96e3-6751d44f3fe9\nDeleting Document Id: 8cecfe88-5ab2-4168-8f05-3c0d20c9577c\nDeleting Document Id: 83085f3d-3758-401d-8116-137bc182c1a3\nDeleting Document Id: cd33616b-ed44-42f0-829a-bb5dc7f07552\nDeleting Document Id: f11f7007-c9ee-429f-9f51-f72c645aede0\nDeleting Document Id: eb632ebc-ee0a-4fb0-8bb5-e2ec3c667e5d\nDeleting Document Id: 818c361d-9edd-4341-b995-854568fb9e56\nDeleting Document Id: 4926fd0a-c042-4143-a4ab-8ba47d0ff5e8\nDeleting Document Id: d4ff61c5-d676-4296-8dd6-b0a5eb997c23\nDeleting Document Id: 070d2f90-2748-4900-811a-dc568c9f2c39\nDeleting Document Id: f84f6f79-c85e-49dd-b5b1-14e3383a544b\nDeleting Document Id: b74746de-60aa-40e0-b9c5-fa8a538b8e98\nDeleting Document Id: b14c6cb2-c71f-4338-aa84-a9e916d72b4a\nDeleting Document Id: 43f27937-db34-4456-a988-a49159926dd0\nDeleting Document Id: a5bfc6f3-25a1-4512-b3bb-a8d46c31956b\nDeleting Document Id: 0e321ff1-222f-448d-be99-00cbd7f02602\nDeleting Document Id: 0bacc8fe-7490-4d26-a3f1-31e5fd0ab9a1\nDeleting Document Id: 570fd7c3-f19f-4ae4-a86e-15cc4ec248b9\nDeleting Document Id: bd79643d-b7dc-4369-81b2-2d087567afe8\nDeleting Document Id: 290d390a-812a-4f34-8c5b-e0c18dd3b4ea\nDeleting Document Id: 21f3578f-4da3-45c8-83d0-9cb241318151\nDeleting Document Id: 7e043639-5b0e-410f-9993-73460ede2fee\nDeleting Document Id: d47acc8c-069b-4525-8878-e6ec38c5b273\nDeleting Document Id: cf04da78-7bbd-4139-a786-6ad0481fd50e\nDeleting Document Id: 985b0198-5976-41ec-9900-0b53a6b32a3c\nDeleting Document Id: 52bb0ef2-034c-4159-a1fe-60df05f7a8e5\nDeleting Document Id: 09492afd-8b7c-459f-bd43-2c9079faccaf\nDeleting Document Id: b535f989-136e-4054-8f92-26feaecfc61f\nDeleting Document Id: 4819f944-9a19-473f-a69a-6bc60b298af5\nDeleting Document Id: 1ac294af-eaaa-4b70-9fbe-abc73a377eb0\nDeleting Document Id: e2b58fba-bc9c-49ec-99a7-70a80b8a2ea0\nDeleting Document Id: d14594e4-3521-421e-90c3-8787eef59143\nDeleting Document Id: a8206a11-1821-4bb8-b093-ace2fdda05c5\nDeleting Document Id: 96051fa9-7d11-4141-9cc8-f03c53e111ff\nDeleting Document Id: c4518c97-8d8c-47bf-9df1-f4c957fca3fa\nDeleting Document Id: 6cecfa71-aff6-44fb-9f0c-77bc99107110\nDeleting Document Id: 36c1b4c0-dfaf-4e3b-ae76-07d655a05e84\nDeleting Document Id: f451ab6f-3f06-4436-ba28-a6d25ccfb55a\nDeleting Document Id: d0741e59-86a5-4816-9439-c71f6dc9bc5d\nDeleting Document Id: 7b1069cf-54f7-4fea-bc34-d8a1fb8a1726\nDeleting Document Id: cc703e5c-4f5a-4b0a-bb94-84390f8aa630\nDeleting Document Id: 4806657f-8dc0-4712-9070-ffff5291cd52\nDeleting Document Id: 5c54dfdc-0df3-4028-b1f5-f0dee93410a8\nDeleting Document Id: 5aaa1940-9ec6-4fbc-8d4b-ae498ce4b37e\nDeleting Document Id: 546b4eae-e85a-44a5-82cd-55548dcfe544\nDeleting Document Id: 91db96a9-42b4-4220-970f-690758552366\nDeleting Document Id: c786015e-67fe-47c6-91b9-daac178dc739\nDeleting Document Id: 4759eb59-b0d7-43b9-87eb-465bebc036cb\nDeleting Document Id: 851b8a1f-3253-448e-a97e-0501539d2657\nDeleting Document Id: cf7c2180-c708-4458-b0b7-f4730db5b1e3\nDeleting Document Id: 3b559c83-97ed-4061-bc6a-30364d400e4f\nDeleting Document Id: 53f53bf5-da2c-4cc7-878c-72b642bf1603\nDeleting Document Id: 2eb052b3-bf6d-4c9c-8ee2-c53562ae6cec\nDeleting Document Id: 9d2c4058-313c-4c3a-9879-748d24c1d27e\nDeleting Document Id: e178c7b2-20e5-4355-9204-669026b89891\nDeleting Document Id: 4c6fa535-53af-4372-b9f9-d9699b9f40d1\nDeleting Document Id: 0c5033d7-dadf-4cc5-8d78-2dacc159a7dc\nDeleting Document Id: 9f5a919d-cfb9-4810-9abb-649dc916b9bf\nDeleting Document Id: 85c61685-b8b1-42e3-ad8a-878d0e2be4ab\nDeleting Document Id: eccdca45-2fda-4ae0-87d3-4267d0c2e491\nDeleting Document Id: 4e559000-2f80-4a96-a716-f17436910078\nDeleting Document Id: c23c0883-30d9-4833-87e2-7f4166ad2b48\nDeleting Document Id: b3e114e9-8410-4d25-8472-2c6ac5d785f5\nDeleting Document Id: cb501346-ecb4-4f66-8bf3-63b809906abc\nDeleting Document Id: b4772399-5eee-4721-ac1b-3ff95995a564\nDeleting Document Id: 04747d89-af60-439a-a5e1-507b71fca59a\nDeleting Document Id: 28139370-0036-4446-b1cf-1ce5131a7799\nDeleting Document Id: 57904156-c331-4ba2-8a06-a995113357cc\nDeleting Document Id: a78a8d67-b286-4c62-9ca8-742565248644\nDeleting Document Id: f4933231-a32c-4dbd-a753-42bd9c6cd937\nDeleting Document Id: ba7be899-9156-44fa-bb34-93daa2fa8e47\nDeleting Document Id: 79e8d68a-6d0b-4ec5-aaf1-020e169bc132\nDeleting Document Id: 4be13be5-31a3-4fe8-a08c-037fa7ca60c9\nDeleting Document Id: 9fb87be5-3103-40b0-b4cd-d99fac46d86b\nDeleting Document Id: 037a53be-a119-404a-b621-0a63fa6d2943\nDeleting Document Id: d60f1d1a-1ebb-4806-9b3e-fddbc855acf6\nDeleting Document Id: 20acfa6f-485a-4160-a61e-28164a22f87d\nDeleting Document Id: 3cbd6962-672d-4fc7-8394-a23d51970b26\nDeleting Document Id: 58b626da-018b-4c99-a006-3573783713a5\nDeleting Document Id: 8592e943-6d55-4b60-a446-9535d857a41d\nDeleting Document Id: fda2a737-954a-4866-820d-516b04bf28b0\nDeleting Document Id: 714ad728-814c-4dd8-8ea9-92af1d597168\nDeleting Document Id: 75cb63d8-13e8-49af-bf44-e20d0fd4abca\nDeleting Document Id: b969d301-5bd5-4341-a7a3-070cb6e40838\nDeleting Document Id: f078feb4-8965-4dcc-b778-5b238185c59d\nDeleting Document Id: bee2cd5e-6e79-435b-8e76-59750975ba86\nDeleting Document Id: fc96f4d4-da36-47a5-bc87-d63a60988f73\nDeleting Document Id: 40e42d6d-7d0c-4ad5-a6ca-8fb31c733055\nDeleting Document Id: 739b09fc-09b3-4c21-90d3-d769982e8023\nDeleting Document Id: 6335e000-839e-46d3-b182-2070484efc26\nDeleting Document Id: da2c8d98-2b48-4092-bf3d-26f62f4aeccf\nDeleting Document Id: 766c6edd-430a-4b3a-81d1-5ed93bccaa89\nDeleting Document Id: 9df1ac14-03f2-4169-bd12-d1378ba5a4bd\nDeleting Document Id: d3d9abe3-94e4-4ed4-8092-100b65681c7b\nDeleting Document Id: 35346422-74df-4c99-a9bd-af55266986fe\nDeleting Document Id: 374ac9eb-a9b2-41b9-ba22-7451817f79f5\nDeleting Document Id: 6ccaeb36-20b7-4eec-b6c6-f6722f55cb7e\nDeleting Document Id: 4b7bda63-dab7-40ec-8f7a-6c7fd65b1829\nDeleting Document Id: 186e75c4-7b12-454d-9071-49222da473ff\nDeleting Document Id: 7d665d5e-2f7c-44dd-a075-6c8109a6185e\nDeleting Document Id: 5c37a169-d8b7-4d25-8ba1-93b176fac3f0\nDeleting Document Id: b0e86194-309a-4d41-876b-d95039b3b5ad\nDeleting Document Id: 8fda4084-f6a2-4b2e-9cc8-6be22e9f5ff7\nDeleting Document Id: c4885231-9bba-4124-98fc-8145699e0edc\nDeleting Document Id: e25f44d5-4f6d-47e8-b2a7-6b8cab34779e"
          },
          "execution_count": 45,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# delete any existing maintenance predictions\n",
        "from azure.cosmos import CosmosClient, PartitionKey, exceptions\n",
        "\n",
        "client = CosmosClient(endpoint, credential=masterkey)\n",
        "database = client.get_database_client(\"ContosoAuto\")\n",
        "container = database.get_container_client(\"maintenance\")\n",
        "\n",
        "for item in container.query_items(query='SELECT * FROM c',\n",
        "                                  enable_cross_partition_query=True):\n",
        "    print('Deleting Document Id: {0}'.format(item['id']))\n",
        "    container.delete_item(item, partition_key=item['vin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  'JavaPackage' object is not callable\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true."
          },
          "execution_count": 46,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# write the new prediction out to Cosmos DB\n",
        "batch_predictions = spark.createDataFrame(batch_predictions_pdf)\n",
        "batch_predictions.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"overwrite\").options(**writeConfig).save()"
      ]
    }
  ]
}