{
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Scoring data\n",
        "In this notebook, you will use a forecasting model to determine if the battery will need replacement within the next 30 days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "import azureml\n",
        "from azureml.core import Run\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "import scipy\n",
        "\n",
        "# Verify versions of key libraries\n",
        "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
        "print(\"Azure ML SDK Version:\", azureml.core.VERSION)\n",
        "print(\"SciPy Version: \", scipy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variables\n",
        "\n",
        "First, follow the steps in the \"Setup Service Principal Authentication\" link in the next cell. Get the output values, as described in the next cell, and paste them into the following cell with all the variables for this notebook.\n",
        "\n",
        "Second, provide the remaining needed values for the variables which will be used throughout the rest of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure access to the Azure Machine Learning resources\n",
        "\n",
        "## IMPORTANT - ACTION REQUIRED: Configure Service Principal authentication following the instructions here: [Setup Service Principal Authentication](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?view=azure-ml-py#set-up-service-principal-authentication).\n",
        "\n",
        "Use the JSON output from the commands in the above link to retrieve the values needed for `tenant_id`, `service_principal_id`, and `service_principal_password` in the next cell.\n",
        "\n",
        "Note: if the Azure account you are using has access to multiple Azure subscriptions, **make sure you run CLI commands in the correct Azure subscription**. You can set the default subscription to the one you are using for the lab/demo with the Azure CLI command `az account set`.\n",
        "\n",
        "Reference: https://docs.microsoft.com/cli/azure/account#az-account-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Provide the Subscription ID of the Azure subscription you are using for the lab/demo\n",
        "subscription_id = \"\"\n",
        "\n",
        "# Resource Group name where your lab/demo resources are deployed\n",
        "resource_group = \"\"\n",
        "\n",
        "# Azure Machine Learning Workspace name and Azure region\n",
        "# Get these from the Azure ML workspace Overview in your Resource Group\n",
        "workspace_name = \"\"\n",
        "workspace_region = \"\"\n",
        "\n",
        "# Values from `Setup Service Principal Authentication` in the above cell\n",
        "tenant_id = \"\" # Use \"tenantId\" value\n",
        "service_principal_id = \"\" # Use \"clientId\" value\n",
        "service_principal_password = \"\" # Use \"clientSecret\" value\n",
        "\n",
        "# Pre-trained ML model\n",
        "# Update for final release\n",
        "# pkl_url = \"https://github.com/AzureCosmosDB/scenario-based-labs/blob/master/IoT/deploy/modelv3.pkl?raw=true\"\n",
        "pkl_url = \"https://github.com/plzm/scenario-based-labs/blob/iot-2020/IoT/deploy/modelv3.pkl?raw=true\"\n",
        "local_folder = \"models\"\n",
        "local_path = local_folder+\"/modelv3.pkl\"\n",
        "model_name = \"batt-cycles-7\"\n",
        "\n",
        "# Cosmos DB\n",
        "# Change this to the Azure region to which you deployed your lab/demo\n",
        "cosmos_db_region = \"\"\n",
        "cosmos_db_database = \"ContosoAuto\"\n",
        "cosmos_db_container_metadata = \"metadata\"\n",
        "cosmos_db_container_maintenance = \"maintenance\"\n",
        "\n",
        "synapse_cosmos_db_linked_service = \"CosmosDbIoTLab\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "\n",
        "sp = ServicePrincipalAuthentication(\n",
        "    tenant_id=tenant_id,\n",
        "    service_principal_id=service_principal_id,\n",
        "    service_principal_password=service_principal_password)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.get(\n",
        "    name=workspace_name, \n",
        "    auth=sp,\n",
        "    subscription_id=subscription_id)\n",
        "\n",
        "ws.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the pre-trained model\n",
        "A pre-trained model has been made available in a public Azure Storage account; see variables above for the URL to that pkl file.\n",
        "\n",
        "Run the next cell to download the model and then register it as a model within your Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from azureml.core import Model\n",
        "\n",
        "print(\"Downloading the pre-trained model...\")\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "urllib.request.urlretrieve(pkl_url, local_path)\n",
        "\n",
        "print(\"Download complete.\")\n",
        "\n",
        "print(\"Uploading and registering model...\")\n",
        "registered_model = Model.register(\n",
        "    model_path=local_path, \n",
        "    model_name=model_name, \n",
        "    workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following to retrieve the model from your Azure Machine Learning workspace, and inspect some of its properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.model import Model\n",
        "from sklearn.externals import joblib\n",
        "from azureml.train import automl\n",
        "\n",
        "model_path = Model.get_model_path(model_name=model_name, _workspace=ws)\n",
        "print(\"Model saved to \", model_path)\n",
        "model = joblib.load(model_path)\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data from Cosmos DB to batch score it\n",
        "Run the following cells to query Cosmos DB Analytical store, prepare the data using SQL queries and then surface the data as temporary views."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Temp View\n",
        "Now we register the view required to create the dataset that will be used to make the predictions. Notice how you are now capable to join data from multiple Cosmos DB containers.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# vehicle_metadata_df = spark.read.cosmos_olap('metadata').createOrReplaceTempView(\"metadata\")\n",
        "\n",
        "vehicle_metadata_df = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", synapse_cosmos_db_linked_service)\\\n",
        "    .option(\"spark.cosmos.container\", cosmos_db_container_metadata)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(vehicle_metadata_df.count())\n",
        "\n",
        "vehicle_metadata_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicle_metadata_df.createOrReplaceTempView(\"metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Scoring dataset\n",
        "Now we are ready to use the previously created view to generate the final dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "trips_clean = spark.sql(\"\"\"\n",
        "    SELECT  vin, \n",
        "            to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripEnded, \n",
        "            to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripStarted, \n",
        "            ((unix_timestamp(to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")) - \n",
        "                unix_timestamp(to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")))/60.0) as tripDurationMinutes\n",
        "    FROM metadata\n",
        "    WHERE entityType = 'Trip' AND status = 'Completed'\n",
        "    \"\"\")\n",
        "\n",
        "trips_clean.createOrReplaceTempView(\"trips_clean\")\n",
        "\n",
        "print(trips_clean.count())\n",
        "trips_clean.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicles_raw = spark.sql(\"\"\"\n",
        "    SELECT vin, batteryAgeDays, batteryRatedCycles, lifetimeBatteryCyclesUsed \n",
        "    FROM metadata \n",
        "    WHERE entityType ='Vehicle'\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_raw.createOrReplaceTempView(\"vehicles_raw\")\n",
        "print(vehicles_raw.count())\n",
        "vehicles_raw.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicles_batch = spark.sql(\"\"\"\n",
        "    SELECT  v.vin as vin, \n",
        "            to_date(t.tripEnded, 'yyyy-MM-dd') as tripEnded, \n",
        "            t.tripDurationMinutes, \n",
        "            v.batteryAgeDays, \n",
        "            v.batteryRatedCycles, \n",
        "            v.lifetimeBatteryCyclesUsed \n",
        "    FROM    vehicles_raw v \n",
        "    INNER JOIN trips_clean t \n",
        "        ON v.vin = t.vin\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_batch.createOrReplaceTempView(\"vehicles_batch\")\n",
        "print(vehicles_batch.count())\n",
        "vehicles_batch.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Let's take a look at our dataframe\r\n",
        "\r\n",
        "vehicles_batch.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run the following cells to convert the Spark DataFrame to a Pandas DataFrame for use with the pre-created model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "spark_df = spark.sql(\"\\\n",
        "    SELECT\\\n",
        "        vin,\\\n",
        "        cast(tripEnded as string) as date,\\\n",
        "        tripDurationMinutes as daily_Trip_Duration,\\\n",
        "        batteryAgeDays as battery_Age_Days,\\\n",
        "        batteryRatedCycles,\\\n",
        "        lifetimeBatteryCyclesUsed\\\n",
        "    FROM vehicles_batch v\")\n",
        "\n",
        "pd_df = spark_df.toPandas()\n",
        "\n",
        "pd_df['date'] = pd.to_datetime(pd_df['date']) # Added to address Spark Date to Pandas date conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the scoring logic\n",
        "The following cell will apply the model and return a prediction for whether or not maintenance is required.\n",
        "\n",
        "Run the following cell to define the helper method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "def predict_maintenance(row):\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "\n",
        "    predict_needs_service = 0\n",
        "\n",
        "    start_day = row[\"battery_Age_Days\"]\n",
        "    daily_trip_duration = row[\"daily_Trip_Duration\"]\n",
        "    current_cycles = row[\"lifetimeBatteryCyclesUsed\"]\n",
        "    rated_lifetime_cycles = row[\"batteryRatedCycles\"]\n",
        "\n",
        "    # In this data, we don't have a time series; we have one trip. So we simulate a time series by\n",
        "    # using the daily trip duration and creating a 30-day series with that. It's a flat line, since\n",
        "    # each day will see the identical daily trip duration being used, but we want to show a time series\n",
        "    # being used here.\n",
        "    dayslist = range(start_day, start_day + 30)\n",
        "    records = [{'battery_Age_Days': x, 'daily_Trip_Duration': daily_trip_duration} for x in dayslist]\n",
        "\n",
        "    pds_df = pd.DataFrame(records)\n",
        "\n",
        "    reg = LinearRegression().fit(pds_df[[\"battery_Age_Days\"]], pds_df[\"daily_Trip_Duration\"])\n",
        "\n",
        "    y_Pred = reg.predict(np.array(pds_df[[\"battery_Age_Days\"]]))\n",
        "\n",
        "    total_cycles_next_30_days = y_Pred[[29,]][0]\n",
        "\n",
        "    if current_cycles + total_cycles_next_30_days > rated_lifetime_cycles:\n",
        "        predict_needs_service = 1\n",
        "\n",
        "    # print(predict_needs_service)\n",
        "\n",
        "    return predict_needs_service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Calculate the predictions\n",
        "\n",
        "predictions = pd_df.apply(predict_maintenance, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, run the following cell to examine the prediction by `VIN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "batch_predictions_pdf = pd.DataFrame({\"vin\": pd_df[\"vin\"], \"serviceRequired\":predictions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "batch_predictions_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write the predictions back to Cosmos DB\n",
        "Now you will save the previously created predictions DataFrame back to the `maintenance` collection in Cosmos DB.\n",
        "\n",
        "Run the following cells to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Retrieve connection string and key from LinkService\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "connection_string = token_library.getConnectionString(synapse_cosmos_db_linked_service)\n",
        "matchObj = re.match( r'AccountEndpoint=(.*);Database=(.*);AccountKey=\"(.*)\";', connection_string, re.M|re.I)\n",
        "endpoint = matchObj.group(1)\n",
        "masterkey = matchObj.group(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "read_config_maintenance = {\r\n",
        "    \"Endpoint\" : endpoint,\r\n",
        "    \"Masterkey\" : masterkey,\r\n",
        "    \"Database\" : cosmos_db_database,\r\n",
        "    \"Collection\" : cosmos_db_container_maintenance\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Read existing maintenance records (if any)\r\n",
        "\r\n",
        "maintenance_records = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**read_config_maintenance).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "print(maintenance_records.count())\r\n",
        "maintenance_records.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# The Spark dataframe will be created even though this MAY throw an error about attempted Arrow optimization\r\n",
        "# As of 03-Aug-2020 open issue at Apache: https://issues.apache.org/jira/browse/SPARK-30966\r\n",
        "\r\n",
        "batch_predictions = spark.createDataFrame(batch_predictions_pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Examine the dataframe to make sure previous cell succeeded even if the Arrow error gets thrown\r\n",
        "\r\n",
        "batch_predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# If we had existing maintenance records from Cosmos DB, let's join them to the batch predictions on VIN. This is so we get the Cosmos DB-assigned\r\n",
        "# unique id on each document, and can do an update instead of a redundant insert for the same VIN.\r\n",
        "# If there are no maintenance records, we do not join, so we will not pass an id field, which means Cosmos DB will auto-generate it and insert it with the new document.\r\n",
        "\r\n",
        "if maintenance_records.count() > 0:\r\n",
        "    maintenance_records_to_write = batch_predictions.join(maintenance_records, batch_predictions.vin == maintenance_records.vin).select(batch_predictions[\"*\"],maintenance_records[\"id\"])\r\n",
        "else:\r\n",
        "    maintenance_records_to_write = batch_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "print(maintenance_records_to_write.count())\r\n",
        "maintenance_records_to_write.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "write_config_maintenance = {\r\n",
        "    \"Endpoint\": endpoint,\r\n",
        "    \"Masterkey\": masterkey,\r\n",
        "    \"Database\": cosmos_db_database,\r\n",
        "    \"Collection\": cosmos_db_container_maintenance,\r\n",
        "    \"Upsert\": \"true\"\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "maintenance_records_to_write.write.mode(\"overwrite\").format(\"com.microsoft.azure.cosmosdb.spark\").options(**write_config_maintenance).save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        ""
      ]
    }
  ]
}