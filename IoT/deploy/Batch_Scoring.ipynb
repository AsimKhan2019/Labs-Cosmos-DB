{
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import azureml\n",
        "from azureml.core import Run\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "import scipy\n",
        "\n",
        "# Verify versions of key libraries\n",
        "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
        "print(\"Azure ML SDK Version:\", azureml.core.VERSION)\n",
        "print(\"SciPy Version: \", scipy.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configure access to the Azure Machine Learning resources\n",
        "\n",
        "## Configure Service Principal authentication following the instructions here: [Setup Service Principal Authentication](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?view=azure-ml-py#set-up-service-principal-authentication).\n",
        "\n",
        "Use the JSON output from the commands in the above link to retrieve the values needed for `tenant_id`, `service_principal_id`, and `service_principal_password` in the next cell.\n",
        "\n",
        "Note: if the Azure account you are using has access to multiple Azure subscriptions, **make sure you run CLI commands in the correct Azure subscription**. You can set the default subscription to the one you are using for the lab/demo with the Azure CLI command `az account set`.\n",
        "\n",
        "Reference: https://docs.microsoft.com/cli/azure/account#az-account-set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Variables\n",
        "\n",
        "Provide values for the following variables which will be used throughout the rest of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Provide the Subscription ID of the Azure subscription you are using for the lab/demo\n",
        "subscription_id = \"\"\n",
        "\n",
        "# Resource Group name where your lab/demo resources are deployed\n",
        "resource_group = \"\"\n",
        "\n",
        "# Azure Machine Learning Workspace name and Azure region\n",
        "# Get these from the Azure ML workspace Overview in your Resource Group\n",
        "workspace_name = \"\"\n",
        "workspace_region = \"East US\"\n",
        "\n",
        "# Values from `Setup Service Principal Authentication` in the above cell\n",
        "# For reference, SP name you created (not needed in a variable): pz-ml-auth\n",
        "tenant_id = \"\" # Use \"tenantId\" value\n",
        "service_principal_id = \"\" # Use \"clientId\" value\n",
        "service_principal_password = \"\" # Use \"clientSecret\" value\n",
        "\n",
        "# Pre-trained ML model\n",
        "# Update for final release\n",
        "# pkl_url = \"https://github.com/AzureCosmosDB/scenario-based-labs/blob/master/IoT/deploy/modelv3.pkl?raw=true\"\n",
        "pkl_url = \"https://github.com/plzm/scenario-based-labs/blob/iot-2020/IoT/deploy/modelv3.pkl?raw=true\"\n",
        "local_folder = \"models\"\n",
        "local_path = local_folder+\"/modelv3.pkl\"\n",
        "model_name = \"batt-cycles-7\"\n",
        "\n",
        "# Cosmos DB\n",
        "cosmos_db_region = workspace_region\n",
        "cosmos_db_database = \"ContosoAuto\"\n",
        "cosmos_db_container_metadata = \"metadata\"\n",
        "cosmos_db_container_maintenance = \"maintenance\"\n",
        "\n",
        "synapse_cosmos_db_linked_service = \"CosmosDbIoTLab\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Scoring data\n",
        "In this notebook, you will use a forecasting model to determine if the battery will need replacement within the next 30 days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.authentication import ServicePrincipalAuthentication\n",
        "\n",
        "sp = ServicePrincipalAuthentication(\n",
        "    tenant_id=tenant_id,\n",
        "    service_principal_id=service_principal_id,\n",
        "    service_principal_password=service_principal_password)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# By using the exist_ok param, if the workspace already exists we get a reference to the existing workspace\n",
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.get(\n",
        "    name=workspace_name, \n",
        "    auth=sp,\n",
        "    subscription_id=subscription_id)\n",
        "\n",
        "ws.get_details()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the pre-trained model\n",
        "A pre-trained model has been made available in a public Azure Storage account. Run the following cell to download the model and then register it as a model within your Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from azureml.core import Model\n",
        "\n",
        "print(\"Downloading the pre-trained model...\")\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "urllib.request.urlretrieve(pkl_url, local_path)\n",
        "\n",
        "print(\"Download complete.\")\n",
        "\n",
        "print(\"Uploading and registering model...\")\n",
        "registered_model = Model.register(\n",
        "    model_path=local_path, \n",
        "    model_name=model_name, \n",
        "    workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following to retrieve the model from your Azure Machine Learning workspace, and inspect some of its properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.core.model import Model\n",
        "from sklearn.externals import joblib\n",
        "from azureml.train import automl\n",
        "\n",
        "model_path = Model.get_model_path(model_name=model_name, _workspace=ws)\n",
        "print(\"Model saved to \", model_path)\n",
        "model = joblib.load(model_path)\n",
        "print(\"Model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data from Cosmos DB to batch score it\n",
        "Run the following cells to query Cosmos DB Analytical store, prepare the data using SQL queries and then surface the data as temporary views."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Register Temp View\n",
        "Now we register the view required to create the dataset that will be used to make the predictions. Notice how you are now capable to join data from multiple Cosmos DB containers.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# vehicle_metadata_df = spark.read.cosmos_olap('metadata').createOrReplaceTempView(\"metadata\")\n",
        "\n",
        "vehicle_metadata_df = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", synapse_cosmos_db_linked_service)\\\n",
        "    .option(\"spark.cosmos.container\", cosmos_db_container_metadata)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "print(vehicle_metadata_df.count())\n",
        "\n",
        "vehicle_metadata_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicle_metadata_df.createOrReplaceTempView(\"metadata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Scoring dataset\n",
        "Now we are ready to use the previously created view to generate the final dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "trips_clean = spark.sql(\"\"\"\n",
        "    SELECT  vin, \n",
        "            to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripEnded, \n",
        "            to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\") as tripStarted, \n",
        "            ((unix_timestamp(to_utc_timestamp(tripEnded, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")) - \n",
        "                unix_timestamp(to_utc_timestamp(tripStarted, \\\"yyyy-MM-dd'T'HH:mm:ss.SSSX'Z'\\\")))/60.0) as tripDurationMinutes\n",
        "    FROM metadata\n",
        "    WHERE entityType = 'Trip' AND status = 'Completed'\n",
        "    \"\"\")\n",
        "\n",
        "trips_clean.createOrReplaceTempView(\"trips_clean\")\n",
        "print(trips_clean.count())\n",
        "trips_clean.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicles_raw = spark.sql(\"\"\"\n",
        "    SELECT vin, batteryAgeDays, batteryRatedCycles, lifetimeBatteryCyclesUsed \n",
        "    FROM metadata \n",
        "    WHERE entityType ='Vehicle'\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_raw.createOrReplaceTempView(\"vehicles_raw\")\n",
        "print(vehicles_raw.count())\n",
        "vehicles_raw.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "vehicles_batch = spark.sql(\"\"\"\n",
        "    SELECT  v.vin as vin, \n",
        "            to_date(t.tripEnded, 'yyyy-MM-dd') as tripEnded, \n",
        "            t.tripDurationMinutes, \n",
        "            v.batteryAgeDays, \n",
        "            v.batteryRatedCycles, \n",
        "            v.lifetimeBatteryCyclesUsed \n",
        "    FROM    vehicles_raw v \n",
        "    INNER JOIN trips_clean t \n",
        "        ON v.vin = t.vin\n",
        "    \"\"\")\n",
        "\n",
        "vehicles_batch.createOrReplaceTempView(\"vehicles_batch\")\n",
        "print(vehicles_batch.count())\n",
        "vehicles_batch.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "vehicles_batch.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run the following cells to convert the Spark DataFrame to a Pandas DataFrame for use with the pre-created model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "spark_df = spark.sql(\"\\\n",
        "    SELECT\\\n",
        "        vin,\\\n",
        "        cast(tripEnded as string) as date,\\\n",
        "        tripDurationMinutes as daily_Trip_Duration,\\\n",
        "        batteryAgeDays as battery_Age_Days,\\\n",
        "        batteryRatedCycles,\\\n",
        "        lifetimeBatteryCyclesUsed\\\n",
        "    FROM vehicles_batch v\")\n",
        "pd_df = spark_df.toPandas()\n",
        "pd_df['date'] = pd.to_datetime(pd_df['date']) # Added to address Spark Date to Pandas date conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the scoring logic\n",
        "The following cell will apply the model and return a prediction for whether or not maintenance is required.\n",
        "\n",
        "Run the following cell to define the helper method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def predict_maintenance(row):\n",
        "    # from azureml.train import automl\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "\n",
        "    predict_needs_service = 0\n",
        "\n",
        "    startday = row[\"battery_Age_Days\"]\n",
        "    dailytripduration = row[\"daily_Trip_Duration\"]\n",
        "    current_cycles = row[\"lifetimeBatteryCyclesUsed\"]\n",
        "    rated_lifetime_cycles = row[\"batteryRatedCycles\"]\n",
        "\n",
        "    # Simple arithmetic approach if we do not have specific, variable daily trip duration\n",
        "    cycles_per_day = current_cycles / startday\n",
        "    print(cycles_per_day)\n",
        "    total_cycles_in_30_days = current_cycles + (30 * cycles_per_day)\n",
        "    \n",
        "    if total_cycles_in_30_days > rated_lifetime_cycles:\n",
        "        predict_needs_service = 1\n",
        "\n",
        "    print(predict_needs_service)\n",
        "    # dayslist = range(startday, startday + 30)\n",
        "\n",
        "    # pds_df = pd.DataFrame({'battery_Age_Days': dayslist, 'daily_Trip_Duration': dailytripduration})\n",
        "\n",
        "    # reg = LinearRegression().fit(dayslist, pds_df)\n",
        "\n",
        "    # y_Pred = reg.predict(np.array(pds_df))\n",
        "    # total_cycles_next_30_days = y_Pred[[29,]][0][0]\n",
        "\n",
        "    # if current_cycles + total_cycles_next_30_days > rated_lifetime_cycles:\n",
        "    #     predict_needs_service = 1\n",
        "\n",
        "    return predict_needs_service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Calculate the predictions\n",
        "\n",
        "predictions = pd_df.apply(predict_maintenance, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, run the following cell to examine the prediction by `VIN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "batch_predictions_pdf = pd.DataFrame({\"vin\": pd_df[\"vin\"], \"serviceRequired\":predictions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "batch_predictions_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write the predictions back to Cosmos DB\n",
        "Now you will save the previously created predictions DataFrame back to the `maintenance` collection in Cosmos DB.\n",
        "\n",
        "Run the following cells to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Retrieve connection string and key from LinkService\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.getOrCreate()\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
        "\n",
        "connection_string = token_library.getConnectionString(synapse_cosmos_db_linked_service)\n",
        "matchObj = re.match( r'AccountEndpoint=(.*);Database=(.*);AccountKey=\"(.*)\";', connection_string, re.M|re.I)\n",
        "endpoint = matchObj.group(1)\n",
        "masterkey = matchObj.group(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# The Spark dataframe will be created even though this may throw an error about attempted Arrow optimization\r\n",
        "# As of 03-Aug-2020 open issue at Apache: https://issues.apache.org/jira/browse/SPARK-30966\r\n",
        "\r\n",
        "batch_predictions = spark.createDataFrame(batch_predictions_pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "batch_predictions.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "write_config_maintenance = {\r\n",
        "    \"Endpoint\": endpoint,\r\n",
        "    \"Masterkey\": masterkey,\r\n",
        "    \"Database\": cosmos_db_database,\r\n",
        "    \"Collection\": cosmos_db_container_maintenance,\r\n",
        "    \"Upsert\": \"true\"\r\n",
        "}\r\n",
        "\r\n",
        "batch_predictions.write.mode(\"overwrite\").format(\"com.microsoft.azure.cosmosdb.spark\").options(**write_config_maintenance).save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "read_config_maintenance = {\r\n",
        "    \"Endpoint\" : endpoint,\r\n",
        "    \"Masterkey\" : masterkey,\r\n",
        "    \"Database\" : cosmos_db_database,\r\n",
        "    \"Collection\" : cosmos_db_container_maintenance\r\n",
        "}\r\n",
        "\r\n",
        "maint = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**read_config_maintenance).load()\r\n",
        "\r\n",
        "maint.createOrReplaceTempView(\"maintenance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "maint.show()"
      ]
    }
  ]
}