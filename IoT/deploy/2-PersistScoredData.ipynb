{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Variables\r\n",
        "\r\n",
        "# Storage account name for the Synapse WS storage account - starts with \"synsa\"\r\n",
        "storage_acct_name = \"\"\r\n",
        "storage_container_name = \"workspace\"\r\n",
        "storage_path_scored = \"lab-data/scored/\"\r\n",
        "\r\n",
        "# Cosmos DB\r\n",
        "cosmos_db_database = \"ContosoAuto\"\r\n",
        "cosmos_db_container_metadata = \"metadata\"\r\n",
        "cosmos_db_container_maintenance = \"maintenance\"\r\n",
        "\r\n",
        "# Synapse linked service pointing to Cosmos DB Analytical Store - this is where we get the source data\r\n",
        "synapse_cosmos_db_linked_service = \"CosmosDbIoTLab\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sa_uri = \"abfss://workspace@\" + storage_acct_name + \".dfs.core.windows.net/\" + storage_path_scored"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scored_maintenance_df = spark.read.parquet(sa_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(scored_maintenance_df.count())\r\n",
        "\r\n",
        "scored_maintenance_df.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scored_maintenance_df.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write scored metadata back to Cosmos DB maintenance container"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve connection string and key from linked service\r\n",
        "import sys\r\n",
        "import re\r\n",
        "\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "sc = SparkSession.builder.getOrCreate()\r\n",
        "token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
        "\r\n",
        "connection_string = token_library.getConnectionString(synapse_cosmos_db_linked_service)\r\n",
        "matchObj = re.match( r'AccountEndpoint=(.*);Database=(.*);AccountKey=\"(.*)\";', connection_string, re.M|re.I)\r\n",
        "endpoint = matchObj.group(1)\r\n",
        "masterkey = matchObj.group(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "read_config_maintenance = {\r\n",
        "    \"Endpoint\" : endpoint,\r\n",
        "    \"Masterkey\" : masterkey,\r\n",
        "    \"Database\" : cosmos_db_database,\r\n",
        "    \"Collection\" : cosmos_db_container_maintenance\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read existing maintenance records (if any)\r\n",
        "\r\n",
        "existing_maintenance_df = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**read_config_maintenance).load()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(existing_maintenance_df.count())\r\n",
        "\r\n",
        "existing_maintenance_df.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we had existing maintenance records from Cosmos DB, let's join them to the batch predictions on VIN. This is so we get the Cosmos DB-assigned\r\n",
        "# unique id on each document, and can do an update instead of a redundant insert for the same VIN.\r\n",
        "# If there are no maintenance records, we do not join, so we will not pass an id field, which means Cosmos DB will auto-generate it and insert it with the new document.\r\n",
        "\r\n",
        "if existing_maintenance_df.count() > 0:\r\n",
        "    maintenance_records_to_write_df = scored_maintenance_df\\\r\n",
        "        .join(existing_maintenance_df, scored_maintenance_df.vin == existing_maintenance_df.vin)\\\r\n",
        "        .select(scored_maintenance_df[\"*\"], existing_maintenance_df[\"id\"])\r\n",
        "else:\r\n",
        "    maintenance_records_to_write_df = scored_maintenance_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(maintenance_records_to_write_df.count())\r\n",
        "\r\n",
        "maintenance_records_to_write_df.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "write_config_maintenance = {\r\n",
        "    \"Endpoint\": endpoint,\r\n",
        "    \"Masterkey\": masterkey,\r\n",
        "    \"Database\": cosmos_db_database,\r\n",
        "    \"Collection\": cosmos_db_container_maintenance,\r\n",
        "    \"Upsert\": \"true\"\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maintenance_records_to_write_df.write.mode(\"overwrite\").format(\"com.microsoft.azure.cosmosdb.spark\").options(**write_config_maintenance).save()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "saveOutput": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}