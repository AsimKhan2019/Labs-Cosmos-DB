# Cosmos DB scenario-based labs - IoT

## 13. Run batch and real-time predictions

**Duration**: 45 minutes

<!-- TOC -->
- [Task 1: Deploy Batch Inference Pipeline](#task-1-deploy-batch-inference-pipeline)
- [Task 2: Perform Batch Score and write Scored Data back to Cosmos DB](#task-2-perform-batch-score-and-write-scored-data-back-to-cosmos-db)
- [Task 3: Deploy Real Time Inference Pipeline](#task-3-deploy-real-time-inference-pipeline)
- [Task 4: Configure Web App to call Real Time Inference Endpoint](#task-4-configure-web-app-to-call-real-time-inference-endpoint)
<!-- /TOC -->

In this Exercise, you will deploy batch and real-time inferencing endpoints for the ML model you trained in the previous Exercise.

Then you will use the batch inferencing endpoint to perform a batch prediction on the vehicles dataset, and write the predictions back to Cosmos DB.

Then you will configure the web app to call the real-time inferencing endpoint via a REST API call, enabling the real-time predictive capability on the vehicle details view.

### Task 1: Deploy Batch Inference Pipeline

At the top right of the Azure ML design surface, select **Create inference pipeline** > **Batch inference pipeline**.

![Create batch inference pipeline.](../media/aml-infer01.png 'Create batch inference pipeline.')

**IMPORTANT** Ensure you are on the **Batch inference pipeline** tab in the Azure ML Designer.

![Designer batch inference pipeline.](../media/aml-infer01a.png 'Designer batch inference pipeline.')

Drag **Export Data** onto the design surface. Place it below the final **Edit Metadata**. Connect the bottom output of **Edit Metadata** to the top input of **Export Data**.

![Add Export Data.](../media/aml-infer02.png 'Add Export Data.')

Configure **Export Data**.

Select **Datastore type** as **Azure Data Lake Storage Gen2**.

Select **Datastore** and set it to **synsa**.

In **Path**, enter `lab-data/scored/scored.batch.parquet`.

Select **File format** and set it to **Parquet**.

![Configure Export Data.](../media/aml-infer03.png 'Configure Export Data.')

**Export Data** will write the batch-scored data back to Azure Data Lake Store, where you wrote the raw data in Exercise 11. You will write the batch-scored data back to Cosmos DB in Task 2, below.

Now, at the top right of the design surface, select **Submit** and wait for the run to complete. All modules on the design surface will show Completed, and batch-scored data has been written back to Azure Data Lake Store.

(Note that in this lab we are not using **Publish**. However, **Publish** is how to deploy a persistent batch scoring pipeline for later use without requiring the Azure ML Studio and Designer. Consult the Azure ML documentation for additional information on publishing Batch Inferencing pipelines.)

### Task 2: Perform Batch Score and write Scored Data back to Cosmos DB

Leave the Azure ML Designer open in its browser tab. Return to the browser tab with the Azure Portal. In your Resource Group, select the Storage Account whose name begins with **synsa**.

![Select Azure Data Lake Storage Account.](../media/aml-infer04.png 'Select Azure Data Lake Storage Account.')

On the Storage Account's **Overview** blade, select **Containers**.

![Go to Storage Account Containers.](../media/aml-infer05.png 'Go to Storage Account Containers.')

Navigate into the **workspace** container, then into the **lab-data**/**scored** folder and verify that the **scored.batch.parquet** file was saved there by the Batch inferencing pipeline, above.

![Verify batch output file in Azure Data Lake Store.](../media/aml-infer06.png 'Verify batch output file in Azure Data Lake Store.')

Return to your Resource Group. Select the Synapse workspace, then in **Open Synapse Studio** select **Open** to continue this Task in Synapse.

![Go to Synapse Studio.](../media/aml-infer07.png 'Go to Synapse Studio.')

Navigate to **Develop** and open the Synapse notebook named **2-PersistScoredData**. In Exercise 11, Task 5 you were instructed to provide your Storage Account name beginning with **synsa** into the first cell of both notebooks. If you did not do that, ensure that your Storage Account name is now filled into the first cell of the **2-PersistScoredData** notebook.

![Ensure Storage Account name present in 2-PersistScoredData notebook.](../media/aml-infer08.png 'Ensure Storage Account name present in 2-PersistScoredData notebook')

Run the notebook. You can step through it by examining and running each cell individually, or by selecting **Run all** in the top toolbar. Wait until all cells have completed before continuing.

Return to your Resource Group. Navigate into your Cosmos DB account, then to **Data Explorer**. Select the **maintenance** container, then **Items** to display the scored data written back into Cosmos DB by the **2-PersistScoredData** Synapse notebook you just ran. Try selecting some of the data items to find one with a **result** value of **1**, meaning the item was predicted to need maintenance in the next 30 days!

![Navigate to batch data written back to Cosmos DB.](../media/aml-infer09.png 'Navigate to batch data written back to Cosmos DB')

Your work in Synapse Studio is now complete. You can close the browser tab with the Synapse Studio.

You can exit Cosmos DB and return to your Resource Group in the Azure Portal browser tab.

### Task 3: Deploy Real Time Inference Pipeline

### Task 4: Configure Web App to call Real Time Inference Endpoint

