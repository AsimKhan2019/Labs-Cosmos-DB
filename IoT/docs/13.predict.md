# Cosmos DB scenario-based labs - IoT

## 13. Run batch and real-time predictions

**Duration**: 45 minutes

<!-- TOC -->
- [Task 1: Deploy Batch Inference Pipeline](#task-1-deploy-batch-inference-pipeline)
- [Task 2: Perform Batch Score and write Scored Data back to Cosmos DB](#task-2-perform-batch-score-and-write-scored-data-back-to-cosmos-db)
- [Task 3: Deploy Real Time Inference Pipeline](#task-3-deploy-real-time-inference-pipeline)
- [Task 4: Configure Web App to call Real Time Inference Endpoint](#task-4-configure-web-app-to-call-real-time-inference-endpoint)
<!-- /TOC -->

In this Exercise, you will deploy batch and real-time inferencing endpoints for the ML model you trained in the previous Exercise.

Then you will use the batch inferencing endpoint to perform a batch prediction on the vehicles dataset, and write the predictions back to Cosmos DB.

Then you will configure the web app to call the real-time inferencing endpoint via a REST API call, enabling the real-time predictive capability on the vehicle details view.

### Task 1: Deploy Batch Inference Pipeline

At the top right of the Azure ML design surface, select **Create inference pipeline** > **Batch inference pipeline**.

![Create batch inference pipeline.](../media/aml-infer01.png 'Create batch inference pipeline.')

**IMPORTANT** Ensure you are on the **Batch inference pipeline** tab in the Azure ML Designer.

![Designer batch inference pipeline.](../media/aml-infer01a.png 'Designer batch inference pipeline.')

Drag **Export Data** onto the design surface. Place it below the final **Edit Metadata**. Connect the bottom output of **Edit Metadata** to the top input of **Export Data**.

![Add Export Data.](../media/aml-infer02.png 'Add Export Data.')

Configure **Export Data**.

Select **Datastore type** as **Azure Data Lake Storage Gen2**.

Select **Datastore** and set it to **synsa**.

In **Path**, enter `lab-data/scored/scored.batch.parquet`.

Select **File format** and set it to **Parquet**.

![Configure Export Data.](../media/aml-infer03.png 'Configure Export Data.')

**Export Data** will write the batch-scored data back to Azure Data Lake Store, where you wrote the raw data in Exercise 11. You will write the batch-scored data back to Cosmos DB in Task 2, below.

Now, at the top right of the design surface, select **Submit** and wait for the run to complete. All modules on the design surface will show Completed, and batch-scored data has been written back to Azure Data Lake Store.

(Note that in this lab we are not using **Publish**. However, **Publish** is how to deploy a persistent batch scoring pipeline for later use without requiring the Azure ML Studio and Designer. Consult the Azure ML documentation for additional information on publishing Batch Inferencing pipelines.)

### Task 2: Perform Batch Score and write Scored Data back to Cosmos DB

Leave the Azure ML Designer open in its browser tab. Return to the browser tab with the Azure Portal. In your Resource Group, select the Storage Account whose name begins with **synsa**.

![Select Azure Data Lake Storage Account.](../media/aml-infer04.png 'Select Azure Data Lake Storage Account.')

On the Storage Account's **Overview** blade, select **Containers**.

![Go to Storage Account Containers.](../media/aml-infer05.png 'Go to Storage Account Containers.')

Navigate into the **workspace** container, then into the **lab-data**/**scored** folder and verify that the **scored.batch.parquet** file was saved there by the Batch inferencing pipeline, above.

![Verify batch output file in Azure Data Lake Store.](../media/aml-infer06.png 'Verify batch output file in Azure Data Lake Store.')

Return to your Resource Group. Select the Synapse workspace, then in **Open Synapse Studio** select **Open** to continue this Task in Synapse.

![Go to Synapse Studio.](../media/aml-infer07.png 'Go to Synapse Studio.')

Navigate to **Develop** and open the Synapse notebook named **2-PersistScoredData**. In Exercise 11, Task 5 you were instructed to provide your Storage Account name beginning with **synsa** into the first cell of both notebooks. If you did not do that, ensure that your Storage Account name is now filled into the first cell of the **2-PersistScoredData** notebook.

![Ensure Storage Account name present in 2-PersistScoredData notebook.](../media/aml-infer08.png 'Ensure Storage Account name present in 2-PersistScoredData notebook')

Run the notebook. You can step through it by examining and running each cell individually, or by selecting **Run all** in the top toolbar. Wait until all cells have completed before continuing.

Return to your Resource Group. Navigate into your Cosmos DB account, then to **Data Explorer**. Select the **maintenance** container, then **Items** to display the scored data written back into Cosmos DB by the **2-PersistScoredData** Synapse notebook you just ran. Try selecting some of the data items to find one with a **result** value of **1**, meaning the item was predicted to need maintenance in the next 30 days!

![Navigate to batch data written back to Cosmos DB.](../media/aml-infer09.png 'Navigate to batch data written back to Cosmos DB')

Your work in Synapse Studio is now complete. You can close the browser tab with the Synapse Studio.

You can exit Cosmos DB and return to your Resource Group in the Azure Portal browser tab.

### Task 3: Deploy Real Time Inference Pipeline

Return to the Azure ML Studio browser tab you used in Task 1. (If you closed it, navigate to your Resource Group, select the Machine Learning resource, and select **Launch studio**. Navigate to the Designer and into the model you built previously.)

Ensure you have the **Training pipeline** tab selected.

![Azure ML Designer, Training pipeline.](../media/aml-infer10.png 'Azure ML Designer, Training pipeline')

Select **Create inference pipeline** > **Real-time inference pipeline**.

![Azure ML Designer, create Real-time inference pipeline.](../media/aml-infer11.png 'Azure ML Designer, create Real-time inference pipeline')

Ensure you have the **Real-time inference pipeline** tab selected.

![Azure ML Designer, select Real-time inference pipeline tab.](../media/aml-infer12.png 'Azure ML Designer, select Real-time inference pipeline tab')

Before publishing it, you need to make several changes to the real-time inference pipeline.

Select the **Apply SQL Transformation** module that is below the top **maintenance-raw** and **Web Service input** modules. Delete it, and delete the two **Edit Metadata** modules directly below it.

![Delete modules from real-time inference pipeline.](../media/aml-infer13.png 'Delete modules from real-time inference pipeline')

After the modules are deleted, the design surface should resemble this:

![Deleted modules from real-time inference pipeline.](../media/aml-infer14.png 'Deleted modules from real-time inference pipeline')

Drag **Select Columns in Dataset** onto the design surface where the three deleted modules were.

Connect the bottom output of **maintenance-raw** and the bottom output of **Web Service Input** to the top input of **Select Columns in Dataset**.

Connect the bottom output of **Select Columns in Dataset** to the top right input of **Score Model**.

![Add and connect Select Columns in Dataset.](../media/aml-infer15.png 'Add and connect Select Columns in Dataset')

Configure **Select Columns in Dataset**. Select **Edit column** and select the following columns: **tripDurationMinutes**, **batteryAgeDays**, **batteryRatedCycles**, **lifetimeBatteryCyclesUsed**, **vin**.

As you have not run this pipeline yet using **Submit** (at the top right of the design surface), simply paste the list of column names into this module: `tripDurationMinutes,batteryAgeDays,batteryRatedCycles,lifetimeBatteryCyclesUsed,vin`.

![Configure Select Columns in Dataset.](../media/aml-infer16.png 'Configure Select Columns in Dataset')

Below **Score Model**, select and delete **Evaluate Model**. You do not need this for a real-time inference endpoint, and removing it will improve deployed model performance, which is desirable for a real-time inference endpoint that will be called by the web app in response to user input.

![Delete Evaluate Model.](../media/aml-infer17.png 'Delete Evaluate Model')

Next, delete the output connections at the bottom of **Score Model**. Delete the connection between **Select Columns in Dataset** and **Edit Metadata** at the bottom of the design surface. Your design surface should look similar to the following; note the absence of connections below **Score Model**.

![Deleted connections.](../media/aml-infer18.png 'Deleted connections')

Now re-order the modules below **Score Model** as follows.

Drag **Select Columns in Dataset** under **Score Model**. Connect the bottom output of **Score Model** to the top input of **Select Columns in Dataset**.

Drag **Edit Metadata** under **Select Columns in Dataset**. Connect the bottom output of **Select Columns in Dataset** to the top input of **Edit Metadata**.

Drag **Web Service Output** under **Edit Metadata**. Connect the bottom output of **Edit Metadata** to the top input of **Web Service Output**.

Your design surface should now look similar to the following.

![Re-ordered connections.](../media/aml-infer19.png 'Re-ordered connections')

Now configure **Select Columns in Dataset**. Select **Edit column** and select only **Scored Labels**. The real-time inferencing endpoint should return as little data as possible to minimize response time and perceived delay in the calling web application, and the other columns are not needed for output to the calling application.

![Configure Select Columns in Dataset.](../media/aml-infer20.png 'Configure Select Columns in Dataset.')

Select **Edit Metadata** and confirm that only the **Scored Labels** column is selected, and that **New column names** is set to **result**.

![Configure Edit Metadata.](../media/aml-infer21.png 'Configure Edit Metadata.')

The design surface should now look similar to the following.

![Completed Real-time Inference Pipeline on Design Surface.](../media/aml-infer-complete.png 'Completed Real-time Inference Pipeline on Design Surface.')

Now, run the pipeline. Select **Submit** at the top right of the design surface and wait until the run completes.

When the run has completed, select **Deploy** at the top right of the design surface to deploy the model to a real-time endpoint that can be called from other applications.

![Deploy Real-Time Inference Pipeline.](../media/aml-infer-deploy01.png 'Deploy Real-Time Inference Pipeline.')

Select to deploy a new real-time endpoint. Select the compute target you configured in Exercise 12, Task 3. Then select **Deploy**.

![Deploy Real-Time Inference Endpoint.](../media/aml-infer-deploy02.png 'Deploy Real-Time Inference Endpoint.')

Wait until the deployment completes. Then select **view real-time endpoint**.

![View Real-Time Inference Endpoint.](../media/aml-infer-deploy03.png 'View Real-Time Inference Endpoint.')

On the endpoint view, select the **Consume** tab. You will now note two pieces of information that you will use in Task 4, below.

Note (or copy) the **REST endpoint** value: you will use this for the **ScoringUrl** value in Task 4.

Note (or copy) the **Primary key** value: you will use this for the **ScoringKey** value in Task 4.

![Note Real-Time Inference Endpoint REST endpoint URL and Primary key.](../media/aml-infer-deploy04.png 'Note Real-Time Inference Endpoint REST endpoint URL and Primary key.')

### Task 4: Configure Web App to call Real Time Inference Endpoint

In your Resource Group, select the App Service whose name begins with **IoTWebApp-**, then select **Configuration**.

If you used the Demo deployment, then you should see two Application settings whose value you will now set. If you used the Lab deployment or you do not see these two Application settings, add the settings as well as the values.

The Application settings are `ScoringUrl` and `ScoringKey`.

Create or edit of these Application settings and provide the corresponding value from the deployed real-time inference endpoint from the end of Task 3. Then select **Save**, then select **Continue** when prompted.

![Set Web Application Configuration for real-inference endpoint.](../media/aml-infer-webapp01.png 'Set Web Application Configuration for real-inference endpoint.')

Navigate to the App Service's **Overview**. Select the **URL** to open the web application.

![Web Application Overview and URL.](../media/aml-infer-webapp02.png 'Web Application Overview and URL.')

On the web app, navigate to **Vehicles**. Select any vehicle to navigate to its view.

![Navigate to Vehicle.](../media/aml-infer-webapp03.png 'Navigate to Vehicle.')

On the vehicle view, select **Predict battery failure**.

![Predict Vehice Battery Failure.](../media/aml-infer-webapp04.png 'Predict Vehice Battery Failure.')

If the real-time inference pipeline was deployed correctly, and if you correctly set the **ScoringUrl** and **ScoringKey** Application Configuration settings, then the web app will successfully call the deployed real-time inference endpoint and display a predicted maintenance result.

![Predict Vehice Battery Failure Result.](../media/aml-infer-webapp05.png 'Predict Vehice Battery Failure Result.')

This exercise is now completed and you can continue with the next exercise.

[Return to Table of Contents to continue](./README.md)
